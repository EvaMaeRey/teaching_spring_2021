---
title: "Practice time"
output: 
  html_document:
    toc: true
    toc_depth: 2
---


# Setup

We'll use the tidyverse

```{r setup, include=F}
library(tidyverse)
```


# One proportion z test

- Step 1 calculate sample proportion
- Step 2 calc sd for the null:
- Step 3 calc z stat
- Step 4 translate to p-value

> Example: Boxing data...


## Calculating the z-stat and p-value step-by-step

For an overview: check out the slides [here](ma206_lesson_06.html#73)

### Step 1: Calculate the statistic, p-hat.  

```{r theory_p_val_calc}
457 -> num_observations # number of matches

## calculate sample proportion p hat
248 / 457 -> # number of wins for red uniform wearer over total Olympic matches
  sample_proportion

sample_proportion
```


We'll use a little MathJax here too, which translates from plain text to equations which may contain fractions, special characters, sub- and superscripts.  In the MathJax environment below, fenced by dollar signs, **replace the question mark with the calculated statistic, phat.**

$$ \hat{p} = ? $$

```{r}
# I can make a little data frame using tibble
tibble(color_of_winner = c("red", "blue"),
       count = c(248, 457-248)) ->
olympics_matches

olympics_matches


ggplot(data = olympics_matches) +
  aes(x = color_of_winner) +
  aes(y = count) +
  geom_col() +
  labs(title = "In 457 matches there are more winners that were randomly assigned\nto wear red than winners assigned blue") +
  labs(subtitle = "The observed split is not 50/50.  But could it still have just arisen from chance?")
```



### Step 2: Use the null, $\pi$ to think about the distribution under the chance model  


```{r}
## establish pi
.5 -> 
  null_prob
```

**Replace the question mark with the value of $\pi$ for the null hypothesis and alternative hypothesis (this is two-sided).**

$$ H_0:  \pi = ? $$
$$ H_A:  \pi \neq ? $$
**Answer the following question:** Is $\pi$ a *population* parameter? In other words, are we trying to generalize to a population. Or does it describe an underlying rate for a process? 

> Your answer here...

Based on $\pi$ and the number of observations, how much variability is expected under the chance model?  We can use the formula to describe the variability - the standard deviation for the chance model:

$$ sd_{chance} =  \sqrt{\frac{\pi*(1-\pi)}{n}} $$
In R, as follows:

```{r}
## calc sd for the null:
sqrt((null_prob * (1-null_prob)) / num_observations ) ->
  sd_chance_model

sd_chance_model
```

Given that the distribution for the chance model is approximately normal, and what we know about [normal distributions](https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg), we expect ~68% of what's observed just under chance variation to fall within the range...

$$ .5 \pm ? $$


Let's visualize the normal distribution that is implied by the standard deviation we've calculated.

```{r}
tibble(x = 400:600/1000) %>% 
  mutate(density = 
           dnorm(x, 
                 mean = .5, 
                 sd = sd_chance_model)) ->
normal_density_25percent_15observations

ggplot(data = normal_density_25percent_15observations) + 
  aes(x = x) +
  aes(y = density) + 
  geom_area(alpha = .75) + # null model
  geom_vline(xintercept = null_prob) + # expectation under the null
  geom_vline(xintercept = sample_proportion, # what's actually observed
             color = "red") +
  geom_vline(xintercept = .5 - (sample_proportion - .5), # symmetrical distance from pi
             color = "red", linetype = "dashed")
```


### Step 3: Describe how consistent/inconsistent using z-statistic (number of standard deviations from the mean of the null model)

```{r}
## calculate z score
(sample_proportion - null_prob) /
  sd_chance_model ->
z_score

z_score
```

### Step 4: translate z-score to p-value

Using `pnorm()` we translate from the z-score to a p-value, the probability of observing something as extreme or more extreme than the observed outcome, just by chance...

```{r}
# situate in normal distribution 
(1 - pnorm(abs(z_score))) * 2 # we are multiplying times two if two-sided
```



---

## Using `prop.test()` function will do this for you too..

```{r echo = T, comment="", warning=F}
prop.test(x = 248, # number "successes" observed 
          n = 457, # total number observed
          p = .5, # proportion expected under null, pi
          alternative = "two.sided", # which extremes count, other options is "greater" "less" for one sided
          correct = F) # use classic prop test
```



There is quite a lot of output, but you should be familiar with many elements.  What are some of the statistics that look familiar. **Replace question marks below with numerical values or text**

Based on the output:

- $p-value$ = ?
- null probability, $\pi$ = ?
- sample estimate, i.e. the observed proportion, $\hat{p}$ = ?
- alternative hypothesis, $H_A$ = ?

Confirm that the p-value you calculated in the 4 step process is the same as `prop.test()`

> Is it the same? Y/N

**Answer the question**: if my $\alpha$ threshold is .05, is my p-value small enough to reject the null hypothesis?

> your answer here...


Note:

- A value of \alpha = 0.05 implies that the null hypothesis is rejected 5 % of the time when it is in fact true. "Type 1" (false positive) error rate.

- A value of \alpha = 0.01 implies that the null hypothesis is rejected 1 % of the time when it is in fact true. "Type 1" (false positive) error rate.



## prop.test() exercise

Change the values below to see if there is 

```{r echo = T, comment="", warning=F}
prop.test(x = 248, # number "successes" observed 
          n = 457, # total number observed
          p = .5, # proportion expected under null, pi
          alternative = "greater", # which extremes count, other options is "greater" "less" for one sided
          correct = F) # use classic prop test
```




# Continuous variable and the t-statistic



*Now, create a plot using the song lengths data.*

Remember the 


```{r}
# read in data (posted online) of time estimates for time-laps when a 10 second clip is played
read_csv(file = "https://raw.githubusercontent.com/EvaMaeRey/teaching_spring_2021/master/data/chap2.ElapsedTime.csv") %>% 
  rename(time = Time) -> # rename Time as time - this is snake case a coding style that I prefer
  jackson_five

# you've read in the data and created the object:
jackson_five
```

**How many observations are there?**

> your answer here...

```{r}
ggplot(data = jackson_five) + 
  aes(x = time) + # time estimate responses from people
  geom_rug(alpha = .1) + # alpha adjusts transparency of hashes
  geom_histogram() +
  geom_vline(xintercept = 10, # known song snippet length
             linetype = "dashed") +
  geom_vline(xintercept = 30) +
  theme_minimal() +
  labs(title = "Do we have evidence that on average people tend to overestimate \nor underestimatethe Jackson 5 song snippet length?\nOr is what's observed consistent with the null -\ni.e. not overestimating or underestimating on average\n(10 seconds is actually played)")
```

If the mean and median of the data are somewhat different, it is evidence of skew.

```{r}
mean(jackson_five$time)
median(jackson_five$time)
```

**Answer**, Is there some evidence of skew?

>  your answer:  yes/no

If skewed, in what direction?

> **keep the correct response:**  right (mean is > median - outlying, "unusual" values are pulling to the right) or left (mean is < median - outlying, "unusual" values are pulling to the left)



**Task: In the plot above, change "30" in geom_vline(xintercept = 30) to the value of the mean.**

## calculating the t-statistic and pvalue by hand

$$t-statistic = \frac{\bar{x} - \mu}{SE}$$
Where SE is the standard *error*.  The sampling error is our expectation about how far away our mean might be from our true mean for any given sample from a population.  The central limit theorem tells us that for repeated samples, the mean will center on the population mean, and the distribution will be a fraction of the population standard deviation - specifically: the standard deviation divided by the square root of the sample size!  

$$SE = \frac{\sigma}{\sqrt{n}}$$
Sigma is unknown (this is the standard deviation for the population, but we just have a sample), so we use $s$. $s$ denotes the sample standard deviation, in place of $\sigma$, the population standard deviation.

All together:

$$t-statistic = \frac{\bar{x} - \mu}{s/\sqrt{n}}$$

First we calculate the standard error.

```{r}
sd(jackson_five$time)/sqrt(nrow(jackson_five)) ->
  standard_error
```

Then, we need to figure out how far away, in standard errors, our observe mean $\bar{x}$ is from the $\mu$, the parameter (10 seconds).  the t-statistic, like the z-score, is a standardized statistic

```{r}
mean(jackson_five$time) - 10 ->
  observed_v_parameter_difference

observed_v_parameter_difference/standard_error ->
  t_stat

t_stat
```

Complete the following sentence by rounding the t-stat to the nearest whole number:

> The observed mean is about ?? standard errors away from the parameter, 10.  


## Using `t.test()`

```{r}
t.test(x = jackson_five$time, 
       mu = 10)
```
Verify that our t-stat matches the t.test result.

> Does it match? Y/N


Is this standardized statistic, the t-statistic, large enough to justify rejecting the null if my alpha level is .05 (you can look at the p-value too).  

> Yes/No

If my alpha level is .01

> Yes/No


# Confidence intervals

Provide the definition of a confidence interval that the book gives here.  

> 
